---
title: "Reversal 6.27"
author: "Sophie Paolizzi"
date: '2022-06-27'
output: html_document
---

```{r setup, include=FALSE}

pacman::p_load(performance, devtools, tidyverse, nlme, sjPlot, readr, janitor, ggplot2,wesanderson, cowplot, car, merTools, flextable, plotly, emmeans, lme4, qualtRics, dependlab, sjPlot, circular, pracma, cowplot, psych, lattice, mlmRev,rstanarm)

##### Load Data and setwd
ll = FALSE
if (ll == TRUE) {
  home_directory <- "/proj/mnhallqlab/users/sophie/ReversalTask/"
  data_dir <- file.path(paste0(home_directory, "Data/"))
} else {
  home_directory <- "~/github_repos/ReversalTask"
  data_dir <- file.path(paste0(home_directory, "Results/"))
}
load("~/github_repos/ReversalTask/Results/Reversal_models_6.29.RData")

```

### Variables at Play
**prevrt_log**. Reaction time in previous trial. 

**rt_log**. Reaction time of current trial. 

**since_reversal **. Number of trials since a reversal has occurred. Also resets at the beginning of the phase, so counts beginning of block  as a "reversal". 

**task_phase**. Reversal vs. Acquisition.

**prev_choice**. The response participants selected on the previous trial (Acq_stim or Rev_stim). 

**StaySwitch_lag**. Whether participants made the same response or a different one. 

**prevFeedback**. Feedback received from the previous trial - importantly, this is probabilistic."Correct" feedback means participants were told they were correct, where "incorrect" feedback means participants were told they were incorrect. 

### Model Summaries; What's the Same?
#### Stimulus Choice Model (Reversal vs. Acquisition stimuli)
```{r}
summary(choose_rev_3, type = 3)
```

####  Stay/Switch Model (Stick with same or choose new stimulus)
```{r}
car::Anova(stay_switch_4, type = 3)
```

### Previous Choice and Feedback Effects
#### Previous Feedback
If people got correct feedback about their choice, they are *very* likely to choose that stimulus again. However, if they are told they *didnt* choose correctly, they're less likely to choose the Acq stim and even less likely to choose the Reversal stim. Consistent with a win-stay lose-switch strategy.


#### Previous Choice

Participants are also very clearly tied to the stimulus they chose previously according to the choice model. According to the stayswitch model, they might be slightly more likely to switch away from the reversal stim if they previously chose it. 


#### Interaction
```{r}
plot_model(choose_rev_3, type = "pred", terms = c("prevFeedback"))
plot_model(stay_switch_4, type = "pred", terms = c("prev_choice", "prevFeedback"))
```
Confirming what we saw above, people are slightly more likely to switch away from the reversal stim overall. It seems they stay if there is correct feedback, and switch if the feedback is incorrect. This isn't strongly observable in the switch model; we see it more clearly in the stim choice model.

### Task Phase by Time Since since reversal

#### Task Phase
People are more likely to choose the Acq. stim overall, but more likely to choose the reversal stim on the reversal block. Theyre also marginally more likely to switch in the reversal block.

#### Time since Reversal
People are more likely to choose the Acquisition stimulus over time, and while they have a general tendency toward staying, they are more prone to switching earlier in a block. 


#### Interaction

The interaction between these effects suggests that people tend to learn the "correct" stimulus over time
```{r}
plot_model(choose_rev_3, type = "pred", terms = c("since_reversal", "task_phase"))
plot_model(stay_switch_4, type = "pred", terms = c("since_reversal", "task_phase"))
```
In the stim choice model, we see a small tendency to choose the block-congruent stimuli as we move away from reversal points. However, the choose_A model seems to reflect that individuals are not switching much in the reversal block. 

### Model Summaries; What's Different?

#### Stimulus Choice Model (Reversal vs. Acquisition stimuli)
```{r}
car::Anova(choose_rev_3, type = 3)
```

##### Task Phase and Previous Feedback Interaction
```{r}
plot_model(choose_rev_3, type = "pred", terms = c("task_phase", "prevFeedback"))
```

```{r}
plot_model(choose_rev_3, type = "pred", terms = c("since_reversal", "prev_choice"))
```
This feels important; over time, people tend to strengthen their convictions regarding either the reversal or acquisition stimulus in line with the time since reversal (or block start). 


####  Stay/Switch Model (Stick with same or choose new stimulus)
```{r}
car::Anova(stay_switch_4, type = 3)
```

```{r}
plot_model(stay_switch_4, type = "pred", terms = c("task_phase", "prev_choice"))
```

#### Major Takeaways Re: masters hypotheses

As a reminder, here are the hypotheses at play:

A. In a serial reversal task, there will be a relationship between heightened BPD symptoms (as indexed by the PAI-BOR) and relatively faster detection of contingency reversals, as indicated by fewer incorrect responses post-reversal. These learners will also be less likely to choose the same option consistently across the blocks. 
B.  In a serial reversal task, higher scores on the PSWQ will be associated with relatively slower detection of reversing reward contingencies; Participants with higher PSWQ scores will tend to be consistent in their choices and experience more errors (and less correct answers) in learning post-reversal. 


Overall, it seems people "learn" which stimuli are good and which are bad relatively quickly; their choices tend to be "sticky" in that they select their previously chosen stimuli repeatedly over time. They are also quite sensitive to feedback/reward, employing a win-stay lose-switch strategy readily. Thus far, no effects of PAI or PSWQ scores are apparent; further investigation using computational modeling parameters is warranted to get a better sense of how stochasticity might play into individual differences in behavior in these tasks. 


### Model Selection for Stay/Switch Models
```{r}
table_stay_switch
table_stay_switch_random
```


### Model Selection for Choose Reversal Models
```{r}
table_choose_rev_base
table_choose_rev_random
```





