---
title: "Random_effect_specification"
author: "Sophie Paolizzi"
date: "4/11/2022"
output: html_document
---

```{r setup, include=FALSE}

pacman::p_load(performance, devtools, tidyverse, nlme, sjPlot, readr, janitor, ggplot2,wesanderson, cowplot, car, merTools, flextable, plotly, emmeans, lme4, qualtRics, dependlab, sjPlot, circular, pracma, cowplot, psych, lattice, mlmRev,rstanarm)

##### Load Data and setwd
ll = FALSE
if (ll == TRUE) {
  home_directory <- "/proj/mnhallqlab/users/sophie/Cannon_Task/"
  data_dir <- file.path(paste0(home_directory, "Data/"))
} else {
  home_directory <- "~/github_repos/ReversalTask"
  data_dir <- file.path(paste0(home_directory, "Data/"))
}
load(paste0(home_directory, "/Data/df_with_coefficients.RData"))
```

### Calculate correlations between scores and random effects
```{r cars}
vars <- names(coefficients)
scores <- unique(with_coefs %>% dplyr::select(any_of(vars) | contains(c("PAI", "PSWQ", "STAI", "BFAS","PID")))) %>% dplyr::select(-c(grpvar, subject))

vars <- vars[! vars %in% c('grp', 'grpvar', 'subject')]

score_names <- colnames(with_coefs %>% dplyr::select(contains(c("PAI", "PSWQ", "STAI", "BFAS","PID")))) 

cors <- matrix(1:(length(vars)*length(score_names)), byrow = FALSE,
               nrow = length(vars), ncol = length(score_names),
               dimnames = list(vars, score_names))

for (i in 1:length(vars)){
  j <- vars[i]
    for (k in score_names){
    tmp <- cor(scores %>% dplyr::select(paste0(j)), scores %>% dplyr::select(paste0(k)), use="na.or.complete")
    cors[j,k] <- as.numeric(tmp)            
    }
  }

cors_df <- as.data.frame(cors)
cors <- cors_df %>% as.data.frame %>% tibble::rownames_to_column() %>% 
    tidyr::pivot_longer(-rowname) %>% filter(abs(value) >.2)

```

### Correlation Structure between questionnaires
```{r}
for_questionnaire_cors <- with_coefs %>% dplyr::select(contains(cors$name))
cor_heatmap(for_questionnaire_cors)

#General Neuroticism Measures (pick best fit): BFAS_Tot, STAI_T, STAI_Tot
#Borderline-Type Measures(pick best fit): PAI_AI, PAI_Tot, PID_emo_lab, PID_host
#Other PID Measures: PID_perf, PID_unusual, PID_rt, PID_sus
```

### Base Effects of individual questionnaires
```{r}
mod_list <- list(); mod_formulas <- list()
cors_names <- unique(cors$name)
for (i in 1:length(cors_names)){
    m <- (paste0("ResponseCorrect_num ~ total_trial_z + prevRT_log + task_phase + prevFeedback*task_phase*prevResponse + gen +", cors_names[i], " + (1 + prevFeedback + task_phase| subject:prevResponse)"))
    mod_formulas[[i]] <- formula(m)
    model <- glmer(mod_formulas[[i]], data = data, family = binomial, control = glmerControl(optimizer = "bobyqa"))
  mod_list[[i]] <- model
}
names(mod_list) <- paste0(cors_names)
beepr::beep()
```

### Model Comparison of individual questionnaires 
```{r}
mod_aics <- names(unique(mod_list))
mod_aics$names <- mod_aics$`names(mod_list)`
mod_aics$`names(mod_list)` <- NULL
mod_aics$AICs <- NA
mod_vec <- c()

for(i in mod_list){
  aic <- AIC(i)
  mod_vec <- append(mod_vec, aic)
  }
 mod_aics$AICs <- mod_vec
 
summary(mod_list$PID_callous) #no main effect no fit improvement
summary(mod_list$PID_grnd) #no main effect no fit improvement
#summary(mod_list$STAI_Tot) #no main effect no fit improvement - very nearly unidentifiable
#summary(mod_list$STAI_S) #no main effect no fit improvement - very nearly unidentifiable
summary(mod_list$PID_anh) # drops 1 subject - models are not all fitted to the same number of observations
summary(mod_list$PID_perc_dysreg) # drops 1 subject - models are not all fitted to the same number of observations
summary(mod_list$PID_unusual) # drops 1 subject - models are not all fitted to the same number of observations
summary(mod_list$PID_impuls) #drops 2 subjects - models are not all fitted to the same number of observations
summary(mod_list$PID_sus) #drops 2 subjects -models are not all fitted to the same number of observations
summary(mod_list$PID_rt) #drops 2 subjects - models are not all fitted to the same number of observations
summary(mod_list$PID_disinhib) #drops 2 subjects - models are not all fitted to the same number of observations
#summary(mod_list$PID_psycho) # drops 4 subjects - models are not all fitted to the same number of observations

```

### Effects of task_phase
```{r}
mod_list_task <- list(); mod_formulas_task <- list()
cors_names_task <- cors_names_task[! cors_names_task %in% c('STAI_Tot', 'STAI_S', 'STAI_psycho')]
for (i in 1:length(cors_names)){
    m <- (paste0("ResponseCorrect_num ~ total_trial_z + prevRT_log + task_phase + prevFeedback*task_phase*prevResponse + gen +task_phase*", cors_names[i], " + (1 + prevFeedback + task_phase| subject:prevResponse)"))
    mod_formulas_task[[i]] <- formula(m)
    model <- glmer(mod_formulas_task[[i]], data = data, family = binomial, control = glmerControl(optimizer = "bobyqa"))
  mod_list_task[[i]] <- model
}
names(mod_list_task) <- paste0(cors_names, "*task_phase")
beepr::beep()
```

```{r}
mod_aics_task <- data.frame(1:12)
mod_vec <- c(names(mod_list_task))
mod_aics_task$names <- mod_vec
mod_aics_task$`names(mod_list_task)` <- NULL
mod_aics_task$AICs <- NA
mod_vec_task <- c()

for(i in mod_list_task){
  aic <- AIC(i)
  mod_vec_task <- append(mod_vec_task, aic)
  }
 mod_aics_task$AICs <- mod_vec_task
 
summary(mod_list_task$`PID_callous*task_phase`) #no main effect no fit improvement
summary(mod_list_task$`PID_grnd*task_phase`) #no main effect no fit improvement
#summary(mod_list$STAI_Tot) #no main effect no fit improvement - very nearly unidentifiable
#summary(mod_list$STAI_S) #no main effect no fit improvement - very nearly unidentifiable
summary(mod_list_task$`PID_anh*task_phase`) # potential?
summary(mod_list_task$`PID_perc_dysreg*task_phase`) # drops 1 subject - models are not all fitted to the same number of observations
summary(mod_list_task$`PID_unusual*task_phase`) # drops 1 subject - models are not all fitted to the same number of observations
summary(mod_list_task$`PID_impuls*task_phase`) #drops 2 subjects - models are not all fitted to the same number of observations
summary(mod_list_task$`PID_sus*task_phase`) #drops 2 subjects -models are not all fitted to the same number of observations
summary(mod_list_task$`PID_rt*task_phase`) #drops 2 subjects - models are not all fitted to the same number of observations
summary(mod_list_task$`PID_disinhib*task_phase`) #potential
#summary(mod_list$PID_psycho) # drops 4 subjects - models are not all fitted to the same number of observations
```
### Effects of prevFeedback
```{r}
mod_list_feedback <- list(); mod_formulas_feedback <- list()
cors_names_feedback <- cors_names[! cors_names %in% c('STAI_Tot', 'STAI_S', 'STAI_psycho')]
for (i in 1:length(cors_names_feedback)){
    m <- (paste0("ResponseCorrect_num ~ total_trial_z + prevRT_log + task_phase + prevFeedback*task_phase*prevResponse + gen + prevFeedback*", cors_names_feedback[i], " + (1 + prevFeedback + task_phase| subject:prevResponse)"))
    mod_formulas_feedback[[i]] <- formula(m)
    model <- glmer(mod_formulas_feedback[[i]], data = data, family = binomial, control = glmerControl(optimizer = "bobyqa"))
  mod_list_feedback[[i]] <- model
}
names(mod_list_feedback) <- paste0(cors_names_feedback, "*prevFeedback")
beepr::beep()

mod_aics_feedback <- data.frame(1:10)
mod_vec <- c(names(mod_list_feedback))
mod_aics_feedback$names <- mod_vec
mod_aics_feedback$`names(mod_list_feedback)` <- NULL
mod_aics_feedback$AICs <- NA
mod_vec_feedback <- c()

for(i in mod_list_feedback){
  aic <- AIC(i)
  mod_vec_feedback <- append(mod_vec_feedback, aic)
  }
 mod_aics_feedback$AICs <- mod_vec_feedback

  
summary(mod_list_feedback$`PID_callous*prevFeedback`) #no main effect no fit improvement
summary(mod_list_feedback$`PID_grnd*prevFeedback`) #no main effect no fit improvement
summary(mod_list_feedback$`PID_anh*prevFeedback`) # potential?
summary(mod_list_feedback$`PID_perc_dysreg*prevFeedback`) # drops 1 subject - models are not all fitted to the same number of observations
summary(mod_list_feedback$`PID_unusual*prevFeedback`) # drops 1 subject - models are not all fitted to the same number of observations
summary(mod_list_feedback$`PID_impuls*prevFeedback`) #drops 2 subjects - models are not all fitted to the same number of observations
summary(mod_list_feedback$`PID_sus*prevFeedback`) #drops 2 subjects -models are not all fitted to the same number of observations
summary(mod_list_feedback$`PID_rt*prevFeedback`) #drops 2 subjects - models are not all fitted to the same number of observations
summary(mod_list_feedback$`PID_disinhib*prevFeedback`) #potential

```

### Winning Model
```{r}
winning_mod <- lme(UPt1 ~ 1 + PEt*StaySwitch_fac*cond + prev_UP + sex.y + PAI_AI*StaySwitch_fac + PID_rt + PID_sus + PID_unusual, 
                       weights=varIdent(form=~1|cond*StaySwitch_fac),
                       random = list(ID = pdDiag(form = ~ 0 + PEt + prev_UP), # l2 random slopes by 
                                     cond_StaySwitch_id = pdDiag(form = ~ 1 + PEt + prev_UP)),
                       na.action = na.exclude,
                       data = ful_df, method='ML' )
summary(winning_mod)
mixedup::extract_het_var(winning_mod, scale = 'var')
VarCorr(winning_mod)
AIC(winning_mod) #doesn't seem to improve fit much
report(winning_mod)
save(winning_mod, all_AICS, all_mod_list, cors, cors_df, ful_df, file = paste0(home_directory,  "Analysis/lmer/Individual_differences_results.RData"))
```

