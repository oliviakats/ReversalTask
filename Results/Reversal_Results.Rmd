---
title: "Reversal_Results"
author: "Sophie Paolizzi"
date: "5/6/2022"
output: html_document
---

#### Michael Feedback 6.08.22
1. Have you coded a variable for trial number since reversal? I would probably code this, and -100/this to capture how accuracy scales with trials since the reversal happened. That’s close to your stated hypothesis about sensitivity to reversal.
 - Done, added reversal sensitivty variable

Is previous response the identity of the previous response (like L/R or some other factor) or is it whether the previous response was correct (basically a lagged copy of the DV)?
If it’s the lagged copy, then this is capturing good behavior (choosing good things), but may not capture a preference for a given stimulus (that has a certain identity). Is that how you’re thinking about it?


I am thinking that the current setup of the models is probably capturing a lot of the key variance, but in ways that are hard to interpret and lead to challenges in the ranef corrs. I would consider running models that use stimulus identity as the DV or maybe even use stay/switch as 1/0. Could you send me your data.frame? I could probably prototype a couple of thigns for comparison, even if ultimately my hunch proves not to be useful

To your specific questions, I think the 3-way interaction reflects that previous choice outcome, not identity is in the interaction, so you get a lot of collinearity when people make repeated, good choices.

And that probably spills over into the ranef structure, which is too correlated to be entirely trustworthy.





11:24
I’m generally thinking of models that are closer to:
glmer(choose_A ~ prior_A * prior_reward * task_phase + (1 | subject))





11:26
But I think you should be able to arrive at the same conclusions either way. This is very parallel to the (H)DDM consideratoin of stimulus versus accuracy coding. If you wanted to stay with accuracy coding, I think it would be:
glmer(choose_best ~ switch * prior_reward  * task_phase + (1|subject))

And with the trial regressor, it’s more like
glmer(choose_best ~ switch * prior_reward  * task_phase * stationary_trial + (1|subject))

where that stationary trial is the trial number since either beginning of block (acquisitoin) or post-reversal

Finally, we’ve approached these kinds of analyses before in terms of stay/switch as outcome (as mentioned above). I think this would be:
glmer(stay ~ prior_stay * prior_reward * task_phase * stationary_trial + (1|subject))


### Markdown Purpose and Content

This markdown follows the model selection procedure for the reversal task from the base model through the random effects structure. 

### Variables at Play
**ResponseCorrect_num**. Outcome variable: whether participants selected the outcome with a higher probabilistic change of accuracy.

**prevRT_log**. Reaction time in previous trial. 

**rt_log**. Reaction time of current trial. 

**block_number**. block number (1-5) of the task. 

**trial**. Within-trial block number.

**Total-trial**. number of trials across blocks in the task.

**Task Phase**. Reversal vs. Acquisition.

**Previous Response**. The response participants selected on the previous trial. 

**Previous Feedback**. Feedback recieved from the previous trial - importantly, this is prbabilistic.

**ResponseandFeedbackCategory**. There are 4 possible outcomes combining previous response and previous feedback on any given trial. A participant could respond correctly and recieve congruent feedback (CRCF; correct resp, feedback = "correct"); they could response incorrectly and receieve congruent feedback (IRCF; incorrect resp, feedback = "incorrect"); they could respond correctly and recieve incongruent feedback (CRIF; correct resp, feedback = "incorrect"); or they could respond incorrectly and recieve incorrect feedback (IRIF; incorrect resp, feedback = "correct"). 

### Hypotheses 
As a reminder, here are the hypotheses at play:

A. In a serial reversal task, there will be a relationship between heightened BPD symptoms (as indexed by the PAI-BOR) and relatively faster detection of contingency reversals, as indicated by fewer incorrect responses post-reversal. These learners will also be less likely to choose the same option consistently across the blocks. 
B.  In a serial reversal task, higher scores on the PSWQ will be associated with relatively slower detection of reversing reward contingencies; Participants with higher PSWQ scores will tend to be consistent in their choices and experience more errors (and less correct answers) in learning post-reversal. 

```{r setup, include=FALSE}
pacman::p_load(tidyverse, dplyr, sjPlot, readr, janitor, ggplot2,wesanderson, cowplot, flextable, plotly, emmeans, lme4, qualtRics, sjPlot, circular, pracma, cowplot, psych, lattice)
##### Load Data and setwd
ll = FALSE
if (ll == TRUE) {
  home_directory <- "/proj/mnhallqlab/users/sophie/ReversalTask/"
  data_dir <- file.path(paste0(home_directory, "data/"))
} else {
  home_directory <- "~/github_repos/ReversalTask/"
  data_dir <- file.path(paste0(home_directory, "Data/"))
}
load(file = paste0(data_dir,'base_effects_data.RData'))

data <- df %>% dplyr::filter(rt !=0) %>% dplyr::filter(!isResponseCorrect == 0) %>% filter(subject != 270470)

data$rt_log <- log(data$rt)
data$rt_inv<- -1/1000*(data$rt)
minfo<- list()
load("~/github_repos/Cannon_Task/Analysis/lmer/glmer_mods.Rdata")
data$ResponseCorrect_num <- data$ResponseCorrect_num %>% dplyr::recode("-1" = 0, "1" = 1)
data$trial_z <- scale(data$trial_number)
data$total_trial_z <- scale(data$total_trialnum)
data$prevRT_log <- lag(log(data$rt))
data$prevResponse<- as.factor(data$prevResponse)
data$prevFeedback <- as.factor(data$prevFeedback)
data$task_phase <- as.factor(data$task_phase)
```


### Base model
Start with the basics.... should be an effect of trial number based on tidy_markdown. 
```{r}
minfo[["mod0"]] <- c(fixed="Intercept", l2="Intercept/Subject")
mod0 <- glmer(ResponseCorrect_num ~ 1 + (1|subject), family = binomial, control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 10, data = df)

minfo[["mod1"]] <- c(fixed="Total Trial", l2="Intercept/Subject")
mod1 <- glmer(ResponseCorrect_num ~  total_trialnum + (1|subject), data = df, family = binomial, control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 10)
#note: trial_number is within-block (i.e., 1-40)
minfo[["mod1.1"]] <- c(fixed="block_trial_number, block number", l2="Intercept/Subject")
mod1.1 <- glmer(ResponseCorrect_num ~  trial_number + block_number + (1|subject), data = df, family = binomial, control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 10); summary(mod1.1)
resids <- residuals(mod1.1)
fitted <- fitted(mod1.1)
plot(fitted, resids)
AIC(mod0, mod1, mod1.1)
```

### Rescale trials
Carry total_trial forward, it covers what we need best. Z-scoring within subject as well. 
```{r}
minfo[["mod2"]] <- c(fixed="total_trial_z", l2="Intercept/Subject")
mod2 <- glmer(ResponseCorrect_num ~ total_trial_z + (1 |subject), data = df, family = binomial, control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 10)

AIC(mod2,mod1)
```


### Add in RT
seems trial-wise RT is doing a little - no reason not to include.
```{r}
minfo[["mod3"]] <- c(fixed="total_trial_z, PrevRT", l2="Intercept/Subject")
mod3 <- glmer(ResponseCorrect_num ~ total_trial_z + rt_log + (1 |subject), data = df, family = binomial, control = glmerControl(optimizer = "bobyqa"))
AIC(mod2, mod3)
```

### Add in previous RT
We've got an effect here of previous RT; seems to be a clear winner. 
```{r}
minfo[["mod3.1"]] <- c(fixed="total_trial_z, PrevRT, RT", l2="Intercept/Subject")
mod3.1 <- glmer(ResponseCorrect_num ~ total_trial_z + prevRT_log + rt_log + (1 |subject), data = df, family = binomial, control = glmerControl(optimizer = "bobyqa"))
AIC(mod3, mod3.1)
```


### Task_phase
Some clear improvement! Good news for our hypotheses re: task phase shifts
```{r}
minfo[["mod4"]] <- c(fixed="total_trial_z, PrevRT, RT, task phase", l2="Intercept/Subject")
mod4 <- glmer(ResponseCorrect_num ~ total_trial_z + prevRT_log + rt_log + task_phase + (1 |subject), data = df, family = binomial, control = glmerControl(optimizer = "bobyqa"))
car::Anova(mod4, type=3)
AIC(mod3.1, mod4)
```

### previous feedback (previous error, kind of)
A previous perceived error leads to choosing the wrong stimulus on the current trial. Seems like a basic win-stay lose-switch strategy, but ignoring the probabilistic part. 
```{r}
minfo[["mod5"]] <- c(fixed="total_trial_z, PrevRT, task phase, previous feedback", l2="Intercept/Subject")
mod5 <- glmer(ResponseCorrect_num ~ total_trial_z + prevRT_log + rt_log + task_phase + prevFeedback + (1 |subject), data = df, family = binomial, control = glmerControl(optimizer = "bobyqa"))
car::Anova(mod5, type = 3)
dependlab::render_flex_aictab(minfo=minfo, mod0, mod1, mod1.1, mod2, mod3, mod4, mod5) %>% autofit() 
```

### previous response
I'm thinking of this as a more direct "stickiness" measure. The clear winner here is the model with an interaction between feedback and response (6.1). We starting to see some upward trends in collinearity we should watch out for. 
```{r}
minfo <- list()
minfo[["mod6"]] <- c(fixed="tot_trial_z, PrevRT, RT, phase, prevFeed, prevResp", l2="Intercept/Subject")
mod6 <- glmer(ResponseCorrect_num ~ total_trial_z + prevRT_log + rt_log + task_phase + prevFeedback + prevResponse + (1 |subject), data = df, family = binomial, control = glmerControl(optimizer = "bobyqa"))

minfo[["mod6.1"]] <- c(fixed="tot_trial_z, PrevRT, RT, phase, prevFeed*prevResp", l2="Intercept/Subject")
mod6.1 <- glmer(ResponseCorrect_num ~ total_trial_z + prevRT_log + rt_log + task_phase + prevFeedback*prevResponse + (1 |subject), data = df, family = binomial, control = glmerControl(optimizer = "bobyqa"))
AIC(mod5, mod6, mod6.1)
car::vif(mod6.1)
car::Anova(mod6.1, type = 3)
plot_model(mod6.1, type = "pred", terms = c("prevFeedback", "prevResponse"))
```

### Interaction with phase and feedback
We see some potential here for a phasexfeedback interaction, suggesting that previous feedback in the reversal block is slightly less salient. 
```{r}
minfo[["mod6.2"]] <- c(fixed="tot_trial_z, PrevRT, RT, phase*prevFeed, prevResp*prevFeed", l2="Intercept/Subject")
mod6.2 <- glmer(ResponseCorrect_num ~ total_trial_z + prevRT_log + rt_log  + task_phase + task_phase*prevFeedback + prevFeedback*prevResponse + (1 |subject), data = df, family = binomial, control = glmerControl(optimizer = "bobyqa"))
AIC(mod6.1, mod6.2)
dependlab::render_flex_aictab(minfo=minfo, mod0, mod1, mod1.1, mod2, mod3, mod4, mod5, mod6, mod6.1, mod6.2) %>% autofit() 
car::Anova(mod6.2, type = 3)
```

### 3-way interaction with phase, feedback, and response
Also seems clear we shouldn't hope for a 3-way interaction with task phase, previous feedback, and previous response, collinearity is intense.  

```{r}
minfo[["mod6.3"]] <- c(fixed="tot_trial_z, PrevRT, RT, phase, phase*prevFeed, prevFeed*prevResp", l2="Intercept/Subject")
mod6.3 <- glmer(ResponseCorrect_num ~ total_trial_z + rt_log + prevRT_log + task_phase + task_phase*prevFeedback*prevResponse + (1 |subject), data = df, family = binomial, control = glmerControl(optimizer = "bobyqa")) #very collinear, probably not salvageable
car::vif(mod6.3)
car::Anova(mod6.3, type = 3)
```

### Decomposing 3-way interaction
Pretty close in fit to our previous model, but the interactions are not significant. Maybe makes sense to carry forward 6.2 for now?
```{r}
minfo[["mod6.4"]] <- c(fixed="tot_trial_z, PrevRT, RT, phase, , prevResp*phase, phase*prevFeed, prevFeed*prevResp", l2="Intercept/Subject")
mod6.4 <- glmer(ResponseCorrect_num ~ total_trial_z + prevRT_log + rt_log + task_phase*prevResponse + prevFeedback*task_phase + prevFeedback*prevResponse + (1 |subject), data = df, family = binomial, control = glmerControl(optimizer = "bobyqa"))
car::Anova(mod6.4, type = 3)
dependlab::render_flex_aictab(minfo=minfo, mod6, mod6.1, mod6.2,mod6.3, mod6.4) %>% autofit() 
```


### Attempt to understand 4-way categories: responseAndFeedbackCategory variable....
Before we officially move on, we do have an alternative variable that has 4 levels that sort of embodies the previousresponsexprevious feedback interaction, but it's not working the way I'd like; I don't know of a dummy code that lets me do a 4-way contrast... we should stick with 6.2 for now. We can definitely say the prevresponse*prevfeeback interaction is in: the others seem like they might have potential once we start playing with random effects. 
```{r}
# mod7 <- glmer(ResponseCorrect_num ~ total_trial_z + prevRT_log + rt_log + responseAndFeedbackCategory*task_phase + (1 |subject), data = df, family = binomial, control = glmerControl(optimizer = "bobyqa")) #she's a degenerate Hessian, not worth considering. 
```

For now, I plan to move forward with the 2 2-way interactions of prev_feedbackxprev_response and prev_responsextask_phase 
So some remaining questions here:
1.) Is there something we can do to reduce collinearity and get the 3-way interaction model tractable?

### Key Random Effects
**Subject**. We are interested in within-subject variation, for sure. 

**Previous Response**. Clear "stickiness" effect of previous response - we want to know if this is heterogenous by subject.

**Previous Feedback**. Clear impact of feedback on current choice - we want to know if this is heterogenous by subject. 

**Task Phase**. It is critical that we understand how the task phase manipulation plays into this. 

#### Basic Random Slopes
Seems there are worthwhile random slopes of task phase, previous response, and previous feedback.
```{r}
minfo[["mod8a"]] <- c(fixed="tot_trial_z, PrevRT, phase*prevFeed, prevFeed*prevResp", l2="Intercept + prevResp/Subject")
mod8a <- glmer(ResponseCorrect_num ~ total_trial_z + prevRT_log + rt_log + prevFeedback*task_phase + prevFeedback*prevResponse + (1 + prevResponse |subject), data = data, family = binomial, control = glmerControl(optimizer = "bobyqa"))

minfo[["mod8b"]] <- c(fixed="tot_trial_z, PrevRT, phase*prevFeed, prevFeed*prevResp", l2="Intercept + prevResp+prevFeed/Subject")
mod8b <- glmer(ResponseCorrect_num ~ total_trial_z + prevRT_log + rt_log + prevFeedback*task_phase + prevFeedback*prevResponse + (1 + prevResponse + prevFeedback |subject), data = data, family = binomial, control = glmerControl(optimizer = "bobyqa"))

minfo[["mod8c"]] <- c(fixed="tot_trial_z, PrevRT, phase*prevFeed, prevFeed*prevResp", l2="Intercept + prevResp+prevFeed+task_phase/Subject")
mod8c <- glmer(ResponseCorrect_num ~ total_trial_z + prevRT_log + rt_log + prevFeedback*task_phase + prevFeedback*prevResponse + (1 + prevResponse + prevFeedback + task_phase |subject), data = data, family = binomial, control = glmerControl(optimizer = "bobyqa"))

car::Anova(mod8c, type = 3)
dependlab::render_flex_aictab(minfo=minfo, mod6, mod6.1, mod6.2, mod6.4, mod8a, mod8b, mod8c) %>% autofit() 
car::vif(mod8c)
```

### Examine random effects structure
Very clear correlations between random effects. Seems as though feedback and task phase are negatively correlated,  
```{r}
plot(ranef(mod8c))
VarCorr(mod8c)
```

```{r}
PrevResp_taskphase <- emmeans(mod8c, ~  prevResponse | task_phase, sigmaAdjust = c(TRUE, FALSE))
plot(PrevResp_taskphase)
```


```{r}
PrevFeedback_taskphase <- emmeans(mod8c, ~  prevFeedback | task_phase, sigmaAdjust = c(TRUE, FALSE))
plot(PrevFeedback_taskphase)
```

```{r}
prevResponse_Feedback <- emmeans(mod8c, ~   prevFeedback | prevResponse, sigmaAdjust = c(TRUE, FALSE))
plot(prevResponse_Feedback) #if you get correct 
```
#### Nested slope in task phase
Degenerate hessian if both random slope and nested effect are present. Seem the straight random effects are the winners here. 
```{r}
# mod8 <- glmer(ResponseCorrect_num ~ total_trial_z + prevRT_log + rt_log + prevFeedback*task_phase + prevFeedback*prevResponse + (1 + prevResponse + prevFeedback + task_phase |subject) + (1 | subject:task_phase), data = data, family = binomial, control = glmerControl(optimizer = "bobyqa"))

minfo[["mod8.1"]] <- c(fixed="tot_trial_z, PrevRT, phase*prevFeed, prevFeed*prevResp", l2="Intercept + Intercept+prevResp+prevFeed+task_phase/Subject, Intercept/Subject:task_phase")
mod8.1 <- glmer(ResponseCorrect_num ~ total_trial_z + prevRT_log + rt_log + prevFeedback*task_phase + prevFeedback*prevResponse + (1 + prevResponse + prevFeedback |subject) + (1 | subject:task_phase), data = data, family = binomial, control = glmerControl(optimizer = "bobyqa"))

VarCorr(mod8.1)
dependlab::render_flex_aictab(minfo=minfo, mod6, mod6.1, mod6.2, mod6.4, mod8a, mod8b, mod8c, mod8.1) %>% autofit() 
```

Remaining Questions:

1.) Is the amount of correlation in the random effects structure a problem? 
2.) Is there something we can do to reduce collinearity and get the 3-way interaction model tractable?
