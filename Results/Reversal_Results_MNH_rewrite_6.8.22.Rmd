---
title: "Reversal Choice Models; Fixed Effects"
author: "Sophie Paolizzi"
date: "6/6/2022"
output: html_document
---

#### Michael Feedback 6.08.22
1. Have you coded a variable for trial number since reversal? I would probably code this, and -100/this to capture how accuracy scales with trials since the reversal happened.That’s close to your stated hypothesis about sensitivity to reversal.
- Done, added reversal sensitivity variable. I also scaled it as the difference in scales was causing convergence issues; curious if that has conceptual problems. 

2. Is previous response the identity of the previous response (like L/R or some other factor) or is it whether the previous response was correct (basically a lagged copy of the DV)?
If it’s the lagged copy, then this is capturing good behavior (choosing good things), but may not capture a preference for a given stimulus (that has a certain identity). Is that how you’re thinking about it?

- this is in fact the accuracy of the previous stim. I added a choose_A that captures the stim identity element, and lagged that as well so we can test both.

3.I am thinking that the current setup of the models is probably capturing a lot of the key variance, but in ways that are hard to interpret and lead to challenges in the ranef corrs. I would consider running models that use stimulus identity as the DV or maybe even use stay/switch as 1/0. Could you send me your data.frame? I could probably prototype a couple of things for comparison, even if ultimately my hunch proves not to be useful.

I’m generally thinking of models that are closer to:
glmer(choose_A ~ prior_A * prior_reward * task_phase + (1 | subject))

If you wanted to stay with accuracy coding, I think it would be:
glmer(choose_best ~ switch * prior_reward  * task_phase + (1|subject))

Finally, we’ve approached these kinds of analyses before in terms of stay/switch as outcome (as mentioned above). I think this would be:
glmer(stay ~ prior_stay * prior_reward * task_phase * stationary_trial + (1|subject))

Will run each of these to see how the story changes in each set. Beginning with choose_A, and will sub in the other DVs for the final-ish model. 

4. With the trial regressor, it’s more like
glmer(choose_best ~ switch * prior_reward  * task_phase * stationary_trial + (1|subject))

where that stationary trial is the trial number since either beginning of block (acquisition) or post-reversal

- recoded the "stationary trial" as phase_trialnum, and scaled within phase due to convergence issues. 

6. To your specific questions, I think the 3-way interaction reflects that previous choice outcome, not identity is in the interaction, so you get a lot of collinearity when people make repeated, good choices. And that probably spills over into the ranef structure, which is too correlated to be entirely trustworthy.

Great, thanks for resolving. Makes sense to me. 

### Markdown Purpose and Content

This markdown follows the model selection procedure for the reversal task from the base model through the random effects structure. 

### Variables at Play
**ResponseCorrect_num**. Outcome variable: whether participants selected the outcome with a higher  (70%) probabilistic chanCe of reward.

**prevrt_log**. Reaction time in previous trial. 

**rt_log**. Reaction time of current trial. 

**block_number**. block number (1-5) of the task. 

**trial**. Within-phase trial number.

**since_reversal **. Number of trials since a reversal has occurred. Also resets at the beginning of the phase, so counts beginning of block  as a "reversal". 

**task_phase**. Reversal vs. Acquisition.

**prev_choice**. The response participants selected on the previous trial (Acq_stim or Rev_stim). 

**prevFeedback**. Feedback received from the previous trial - importantly, this is probabilistic."Correct" feedback means participants were told they were correct, where "incorrect" feedback means participants were told they were incorrect. 

**ResponseandFeedbackCategory**. There are 4 possible outcomes combining previous response and previous feedback on any given trial. A participant could respond correctly and receive congruent feedback (CRCF; correct resp, feedback = "correct"); they could respond incorrectly and receieve congruent feedback (IRCF; incorrect resp, feedback = "correct"); they could respond correctly and recieve incongruent feedback (CRIF; correct resp, feedback = "incorrect"); or they could respond incorrectly and recieve incorrect feedback (IRIF; incorrect resp, feedback = "correct"). 

### Hypotheses 
As a reminder, here are the hypotheses at play:

A. In a serial reversal task, there will be a relationship between heightened BPD symptoms (as indexed by the PAI-BOR) and relatively faster detection of contingency reversals, as indicated by fewer incorrect responses post-reversal. These learners will also be less likely to choose the same option consistently across the blocks. 
B.  In a serial reversal task, higher scores on the PSWQ will be associated with relatively slower detection of reversing reward contingencies; Participants with higher PSWQ scores will tend to be consistent in their choices and experience more errors (and less correct answers) in learning post-reversal. 

```{r setup, include=FALSE}
pacman::p_load(tidyverse, dplyr, sjPlot, readr, janitor, ggplot2,wesanderson, cowplot, flextable, plotly, emmeans, lme4, qualtRics, sjPlot, circular, pracma, cowplot, psych, lattice)
##### Load Data and setwd
ll = FALSE
if (ll == TRUE) {
  home_directory <- "/proj/mnhallqlab/users/sophie/ReversalTask/"
  data_dir <- file.path(paste0(home_directory, "data/"))
} else {
  home_directory <- "~/github_repos/ReversalTask/"
  data_dir <- file.path(paste0(home_directory, "Data/"))
}
load(file = paste0(data_dir,'base_effects_data.RData'))

data <- df %>% dplyr::filter(rt !=0) %>% dplyr::filter(!isResponseCorrect == 0) %>% filter(subject != 270470)

data$rt_log <- log(data$rt)
data$rt_inv<- -1/1000*(data$rt)
minfo<- list()
load("~/github_repos/Cannon_Task/Analysis/lmer/glmer_mods.Rdata")

data <- data %>% 
  mutate(trial_z = scale(trial_number),
         total_trial_z = scale(total_trialnum),
         task_phase = as.factor(task_phase),
         prevFeedback = as.factor(prevFeedback),
         phase_trialnum_z = scale(phase_trialnum),
         since_reversal = scale(-100/phase_trialnum),
         ResponseCorrect_num = dplyr::recode(ResponseCorrect_num, "-1" = 0, "1" = 1)) %>% 
  group_by(subject) %>% mutate(prevResponse = as.factor(lag(Response)),
                               prevrt_log = lag(rt_log)) %>% ungroup()

data$stim_num <- as.numeric(data$stim_num)
data$index_correctChoice <- as.numeric(data$index_correctChoice)
data$index_incorrectChoice <- as.numeric(data$index_incorrectChoice)

#stim_ident
data <- data %>% group_by(subject, task_phase_character) %>%
  mutate(stim_num = ifelse(isResponseCorrect == 1, index_correctChoice, index_incorrectChoice)) %>% ungroup() %>%
  mutate(choose_A = case_when(stim_num == index_correctChoice & task_phase_character == "Reversal" ~ "B",
                              stim_num == index_correctChoice & task_phase_character == "Acquisiton" ~ "A",
                              stim_num == index_incorrectChoice & task_phase_character == "Reversal" ~ "A",
                              stim_num == index_incorrectChoice & task_phase_character == "Acquisiton" ~ "B")) 

data$prev_choice <- lag(data$choose_A)

data$StaySwitch <- as.factor(ifelse(data$choose_A != data$prev_choice, "Switch", "Stay"))
data$StaySwitch_lag <-lag(data$StaySwitch)

#random shorter things
data$phase_early <- ifelse(data$phase_trialnum_z <0, "early", "late")
save(data, file = paste0(data_dir, "MNH_rewrite_data.RData"))

```

### Standard trial-level covariates; Trial
According to Michael's edits and beginning analysis from a stimulus_choice perspective; starting with trial; I compared total_trial_z, or trial number across the full task and all blocks, trial_z which is the scaled, within-block trial number, and the within-phase scaled trial number. I realized trial_z is inherently going to have problems with collinearity once task_phase gets introduced, so going with phase_trialnum_z as Michael originally suggested. 
```{r}

mod0 <- glmer(choose_A ~ trial_z + (1 | subject), family = binomial, control = glmerControl(optimizer = "bobyqa"), nAGQ = 10, data = data)

mod0.1 <- glmer(choose_A ~ total_trial_z + (1 | subject), family = binomial, control = glmerControl(optimizer = "bobyqa"), nAGQ = 10, data = data)


mod0.2 <- glmer(choose_A ~ phase_trialnum_z + (1 | subject), family = binomial, control = glmerControl(optimizer = "bobyqa"), nAGQ = 10, data = data)
resids <- residuals(mod0)
fitted <- fitted(mod0)
plot(fitted, resids)
AIC(mod0, mod0.1, mod0.2)
summary(mod0)
```
### Adding in standard trial-level covariates; RT
Adding in the trial RT to the upcoming models - looks like both RT and previous RT aren't, so keeping on-board.
```{r}

mod1 <- glmer(choose_A ~ trial_z + rt_log + (1 | subject), family = binomial, control = glmerControl(optimizer = "bobyqa"), nAGQ = 10, data = data)
summary(mod1)

mod1.1 <- glmer(choose_A ~ trial_z + rt_log + prevrt_log + (1 | subject), family = binomial, control = glmerControl(optimizer = "bobyqa"),
                nAGQ = 10, data = data)

summary(mod1.1)
AIC(mod0, mod1, mod1.1)
car::vif(mod1.1)
```


### Adding in standard trial-level covariates; Task phase
Adding in task block, huge improvement in AIC. Seems to wash out any RT effects that might exist.  
```{r}
mod2 <- glmer(choose_A ~ phase_trialnum_z + rt_log + prevrt_log + task_phase + (1 | subject), family = binomial, control = glmerControl(optimizer = "bobyqa"),
              nAGQ = 10, data = data)
AIC(mod2, mod1.1)
summary(mod2)
car::vif(mod2)
```

### Adding in previous choice
Adding in previous choice,  another big improvement in AIC. 
```{r}
mod3 <- glmer(choose_A ~ prev_choice + phase_trialnum_z + rt_log + prevrt_log + task_phase + (1 | subject), family = binomial, control = glmerControl(optimizer = "bobyqa"),
              nAGQ = 10, data = data)

AIC(mod2, mod3)
summary(mod3)
car::vif(mod3)
```
What I'm understanding is that stim choice and block are in fact related, such that people are more likely (~65%) to choose the Acquisition stim in the acquisition block, and the reversal stim in the reversal block (~59% likely). Because we're working with a binary outcome, I"m treating probability of choosing acquistition as the inverse of the probability of choosing reversal that the model is giving me. Next, we should probably assess temporal/learning dynamics. It seems graphically people are more likely to choose the Acquisition stimulus earlier in the phase, compared to the reversal stimulus late in learning. 
```{r}
histogram(~ choose_A | task_phase, data = data)
emm <- emmeans(mod3, ~ task_phase, type = "response");emm
ggplot(data, aes(x = trial_z, y = task_phase, color = choose_A)) +  geom_point() + geom_jitter()
```



### Emmeans for temporal effects
```{r}
mod3_emmeans <- glmer(choose_A ~ prev_choice*phase_early*task_phase + rt_log + prevrt_log + (1 | subject), family = binomial, control = glmerControl(optimizer = "bobyqa"), nAGQ = 10, data = data) #comparing early vs. late in learning
emm <- emmeans(mod3_emmeans, ~ phase_early*task_phase, type = "response");emm # people stay pretty likely to choose Acq through acq block, probability of choosing reversal seems to increase more throuhgout learning. 

emm <- emmeans(mod3_emmeans, ~ prev_choice*phase_early*task_phase, type = "response");emm #influence of previous choice increases over course of task - indicates learning from previous choice? You see slight strengthening of Acq stim in Acq block, and weakening of Acq in reversal block. Sort of seems like reversal has more bias? 
summary(mod3_emmeans)
car::vif(mod3_emmeans)
```

### Temporal effects: time since since reversal
Some improvement - also seem distanced from reversal (our since_reversal variable) is overtaking the phase_trial number variable. A teeny bump in collinearity there but nothing crazy. 
```{r}
mod4 <- glmer(choose_A ~ prev_choice + phase_trialnum_z + rt_log + prevrt_log + since_reversal + task_phase + (1 | subject), family = binomial, control = glmerControl(optimizer = "bobyqa"),
nAGQ = 10, data = data)
summary(mod4)
car::vif(mod4)
AIC(mod3, mod4)
plot_model(mod4)
```
### Previous reward
previous feedback (correct == earn 5c, incorrect = nothing) seems to have a slight effect, though the previous stimulus choice effect seems much stronger. 
```{r}
mod5 <- glmer(choose_A ~ prev_choice + phase_trialnum_z + rt_log + prevrt_log + since_reversal + task_phase + prevFeedback + (1 | subject), family = binomial, control=glmerControl(optimizer = "bobyqa", optCtrl=list(maxfun=2e4)),
nAGQ = 10, data = data)



summary(mod5)
car::vif(mod5)
AIC(mod4, mod5)
```
### Investigating interactions: previous choice*task phase
Interacting our heavy hitters first; this should give us a sense if people have a harder time adjusting to the new contingency as we hypothesized.  
```{r}
mod6 <- glmer(choose_A ~ prev_choice*prevFeedback + task_phase + phase_trialnum_z + rt_log + prevrt_log + since_reversal + (1 | subject), family = binomial, control=glmerControl(optimizer = "bobyqa", optCtrl=list(maxfun=2e4)),
nAGQ = 10, data = data)


mod6.1 <- glmer(choose_A ~ prev_choice*task_phase*prevFeedback + phase_trialnum_z + rt_log + prevrt_log + since_reversal + (1 | subject), family = binomial, control=glmerControl(optimizer = "bobyqa", optCtrl=list(maxfun=2e4)),
nAGQ = 1, data = data) #vif is just too high, 

summary(mod6.1)
car::vif(mod6); car::vif(mod6.1)
AIC(mod6, mod6.1, mod10)
plot_model(mod6, type = "pred", terms = c("task_phase", "prev_choice"))
```
I'm interpreting this percentage to mean the likelihood that participants choose the reversal stimulus (Acquisition is the reference condition coded as 0). In the Acquisition block, individuals are ~20% likely to choose the Rev_stim  if they chose the Acq previously, indicating they're much more likely to choose the Acq_stim. They are relatively less "sticky" which it comes to choosing the Rev_stim in the Acquisition block; they're closer to 50% likely to choose the Rev_stim in the acquisition block if they chose it previously. 

In the reversal block, people are closer to 75% likely to choose the rev_stim if it was their previous choice. However, they're close to 60% likely to "stick" with the acquisition stimulus in the reversal block. This is potential evidence for perseveration in my view, where interference from the previous contingency leads to choosing the previously conditioned stimulus over again. 

```{r}
emm <- emmeans(mod6, ~ prev_choice+task_phase, type = "response");emm
```

### Investigating interactions: prev_choice x task phase x since_reversal
This is super interesting, but the collinearity might make it intractable. Stick with broken-down 2-ways for now. 
```{r}
mod7 <- glmer(choose_A ~ prev_choice*task_phase*prevFeedback + task_phase*since_reversal + rt_log + prevrt_log + phase_trialnum_z + since_reversal + (1 | subject), family = binomial, control=glmerControl(optimizer = "bobyqa", optCtrl=list(maxfun=2e4)),  nAGQ = 10, data = data)


mod7.0 <- glmer(choose_A ~ prev_choice*task_phase + prev_choice*since_reversal + task_phase*since_reversal  + prev_choice*prevFeedback*task_phase + rt_log + prevrt_log + phase_trialnum_z + since_reversal + (1 | subject), family = binomial, control=glmerControl(optimizer = "bobyqa", optCtrl=list(maxfun=2e4)),
nAGQ = 10, data = data)


mod8 <- glmer(choose_A ~ prev_choice*task_phase + prev_choice*since_reversal + task_phase*since_reversal + prev_choice*prevFeedback + rt_log + prevrt_log + phase_trialnum_z + since_reversal + (1 | subject), family = binomial, control=glmerControl(optimizer = "bobyqa", optCtrl=list(maxfun=2e4)),
nAGQ = 10, data = data)


mod7.1 <- glmer(choose_A ~ prev_choice*task_phase*prevFeedback + task_phase*since_reversal + prevFeedback + rt_log + prevrt_log + phase_trialnum_z + since_reversal + (1 | subject), family = binomial, control=glmerControl(optimizer = "bobyqa", optCtrl=list(maxfun=2e4)),
nAGQ = 10, data = data)

summary(mod7.1)
car::vif(mod7)
AIC(mod7, mod7.0, mod7.1, mod8, mod10)
plot_model(mod7.1, type = "pred", terms = c("since_reversal", "task_phase")) # This makes a lot of sense, shows that people get more likely to choose the reversal stim in the reversal block, and Acq stim in acquisition block. it seems the reversal strengthening effect is significantly weaker than the acq effect, which might be further evidence for the interference/perseveration argument. 

plot_model(mod7.1, type = "pred", terms = c("since_reversal", "prev_choice")) #
```
Choosing Acq_stim previously clearly seems to strengthen association over the time since a reversal. Rev_stim is a little harder to interpret: choosing reversal stim previously actually makes people a little less likely overall to choose it again? I think there's some block effect here that's not being captured.

```{r}
plot_model(mod7.1, type = "pred", terms = c("since_reversal", "prev_choice", "task_phase"))
```
Ok, putting it all together tells a story that makes sense! It seems like people's tendency to choose the reversal pretty much doesn't change (hangs out around 50/50) in the Acquisition block, and participants learn to focus on the Acq stim pretty quickly. In the Reversal block, learning about the reversal block seems to shoot up, while the "unlearning" about the Acquisition stimulus happens much slower.  

I think it also seems bythe end that people are understanding it's good to occasionally sample the acquisition stimulus, as it can be rewarding sometimes (70%). 

### Investigating interactions: previous choice*previous feedback
Huge AIC improvement same as our previous accuracy model; definitely worth adding in. 
```{r}

summary(mod8)
car::vif(mod8)
AIC(mod7, mod8)
plot_model(mod8, type = "pred", terms = c("prev_choice", "prevFeedback"))
```
Ok, so if people got correct feedback about their choice, they are *very* likely to choose that stimulus again. However, if they are told they *didnt* choose correctly, they're only about 40% likely to choose the Acq stim and 60% likely to choose the Reversal stim. I want to look at the since_reversal dymanics of this sooo bad but I'm worried about send the model over the edge in collinearity. Consistent with a win-stay lose-switch strategy.

### Investigating interactions: temporal effects of previous_feedback
Does the influence of previous feedback weaken over time or differ across phases?
```{r}
mod9 <- glmer(choose_A ~ prev_choice*task_phase + prev_choice*since_reversal + task_phase*since_reversal  + prev_choice*prevFeedback + prevFeedback*task_phase + rt_log + prevrt_log + phase_trialnum_z + since_reversal + (1 | subject), family = binomial, control=glmerControl(optimizer = "bobyqa", optCtrl=list(maxfun=2e4)),
nAGQ = 10, data = data) #worried about the collinearity here plus it's not doing much. Drop it I think and see if there's something in the ranef structure

mod9.1 <- glmer(choose_A ~ prev_choice*task_phase + prev_choice*since_reversal + task_phase*since_reversal + prev_choice*prevFeedback*task_phase + prevFeedback*since_reversal + rt_log + prevrt_log + phase_trialnum_z + since_reversal + (1 | subject), family = binomial, control=glmerControl(optimizer = "bobyqa", optCtrl=list(maxfun=2e4)), #not improving fit, we drop
nAGQ = 1, data = data)

car::vif(mod9.1)
AIC(mod8, mod9, mod9.1, mod10)
summary(mod9.1)
plot_model(mod8, type = "pred", terms = c("task_phase", "prevFeedback"))
emm <- emmeans(mod9.0, ~ task_phase*prevFeedback, type = "response");emmeans
```

### random slope of since_reversal 
Just wanted to check if this was salvageable; it's helping considerably with collinearity, so I feel good about keeping my 3-way moving forward. 

```{r}
mod10 <- glmer(choose_A ~ prev_choice*task_phase + task_phase*prevFeedback +  prev_choice*prevFeedback + prev_choice*since_reversal + task_phase*since_reversal + rt_log + prevrt_log + phase_trialnum_z + since_reversal + ( 1 | subject), family="binomial", control=glmerControl(optimizer = "bobyqa", optCtrl=list(maxfun=2e4)), nAGQ = 1, data = data, na.action = na.exclude)
summary(mod10)
car::vif(mod10)
AIC(mod10.1, mod10, mod9, mod7)

vifs <- as.list(vifs)

```


```{r}
plot_model(mod8, type = "pred", terms = c("task_phase"))
```


Estimate Std.  Error    z value  Pr(>|z|)    
(Intercept)                                        -2.00415    0.18029 -11.116  < 2e-16 *** 
prev_choiceRev_stim                                 3.04184    0.06163  49.356  < 2e-16 *** 
#much 
task_phaseReversal                                  0.55029    0.06167   8.923  < 2e-16 ***
#choose acq stim cosistently in 
since_reversal                                     -0.17109    0.03443  -4.970 6.70e-07 ***
prevFeedbackincorrect_feedback                      1.62370    0.05063  32.070  < 2e-16 ***
rt_log                                             -0.06013    0.02862  -2.101  0.03562 *  
prevrt_log                                          0.05559    0.02874   1.935  0.05304 .  
phase_trialnum_z                                    0.02207    0.02018   1.093  0.27423    
prev_choiceRev_stim:task_phaseReversal              0.15871    0.06263   2.534  0.01128 *  
prev_choiceRev_stim:since_reversal                  0.24386    0.03186   7.654 1.94e-14 ***
task_phaseReversal:since_reversal                   0.27934    0.03194   8.745  < 2e-16 ***
prev_choiceRev_stim:prevFeedbackincorrect_feedback -2.91114    0.06487 -44.874  < 2e-16 ***
task_phaseReversal:prevFeedbackincorrect_feedback  -0.20003    0.06467  -3.093  0.00198 ** 



```{r}


modlist <- list()
modlist[["mod0"]] <- mod0
modlist[["mod0.1"]] <- mod0.1
modlist[["mod0.2"]] <- mod0.2
modlist[["mod1"]] <- mod1
modlist[["mod1.1"]] <- mod1.1
modlist[["mod2"]] <- mod2
modlist[["mod3"]] <- mod3
modlist[["mod4"]] <- mod4
modlist[["mod5"]] <- mod5
modlist[["mod6"]] <- mod6
modlist[["mod6.1"]] <- mod6.1
modlist[["mod7"]] <- mod7
modlist[["mod7.1"]] <- mod7.1
modlist[["mod8"]] <- mod8
modlist[["mod9"]] <- mod9
#modlist[["mod9.0"]] <- mod9.0
modlist[["mod9.1"]] <- mod9.1
modlist[["mod10"]] <- mod10
modlist[["mod11"]] <- mod11
modlist[["mod12"]] <- mod12

vifs <- list()
vifs[["mod0"]] <- NA
vifs[["mod0.1"]] <- NA
vifs[["mod0.2"]] <- NA
vifs[["mod1"]] <- max(c(car::vif(mod1)))
vifs[["mod1.1"]] <- max(c(car::vif(mod1.1)))
vifs[["mod2"]] <- max(c(car::vif(mod2)))
vifs[["mod3"]] <- max(c(car::vif(mod3)))
vifs[["mod4"]] <- max(c(car::vif(mod4)))
vifs[["mod5"]] <- max(c(car::vif(mod5)))
vifs[["mod6"]] <- max(c(car::vif(mod6)))
vifs[["mod6.1"]] <- max(c(car::vif(mod6.1)))
vifs[["mod7"]] <- max(c(car::vif(mod7)))
vifs[["mod7.1"]] <- max(c(car::vif(mod7.1)))
vifs[["mod8"]] <- max(c(car::vif(mod8)))
vifs[["mod9"]] <- max(c(car::vif(mod9)))
#vifs[["mod9.0"]] <- max(c(car::vif(mod9.0)))
vifs[["mod9.1"]] <- max(c(car::vif(mod9.1)))
vifs[["mod10"]] <- max(c(car::vif(mod10)))
vifs[["mod11"]] <- max(c(car::vif(mod11)))
vifs[["mod12"]] <- max(c(car::vif(mod12)))

minfo[["mod0"]] <- c(fixed="within-block trial number", ixns = "None", l2="Intercept/Subject",  max_vif = "None")
minfo[["mod0.1"]] <- c(fixed="Total trial number", ixns = "None", l2="Intercept/Subject", max_vif = "None")
minfo[["mod0.2"]] <- c(fixed="Phase-level Trial", ixns = "None", l2="Intercept/Subject", max_vif = "None")
minfo[["mod1"]] <- c(fixed="Trial, RT",ixns = "None",  l2="Intercept/Subject", max_vif = vifs[["mod1"]])
minfo[["mod1.1"]] <- c(fixed="Trial, RT, prevRT", ixns = "None", l2="Intercept/Subject", max_vif = vifs[["mod1.1"]])
minfo[["mod2"]] <- c(fixed="within-phase trial, RT, prevRT, Phase" , ixns = "None", l2="Intercept/Subject", max_vif = (vifs[["mod2"]]))
minfo[["mod3"]] <- c(fixed="Trial, RT, Phase, Previous Choice", ixns = "None", l2="Intercept/Subject", max_vif = as.character(vifs[["mod3"]]))
minfo[["mod4"]] <- c(fixed="Trial, RT, Phase, Previous Choice", ixns = "None", l2="Intercept/Subject", max_vif = as.character(vifs[["mod4"]]))
minfo[["mod5"]] <- c(fixed="Trial, RT, Phase, Previous Choice, trials since reversal", ixns = "None", l2="Intercept/Subject",  max_vif = as.character(vifs[["mod5"]]))
minfo[["mod6"]] <- c(fixed="Trial, RT, Phase, Previous Choice, previous reward, trials since reversal", ixns="Phase x Previous Choice", l2="Intercept/Subject", max_vif = as.character(vifs[["mod6"]]))
minfo[["mod6.1"]] <- c(fixed="Trial, RT, Phase, Previous Choice, previous reward, trials since reversal", ixns="Phase x Previous Choice x previous Feedback", l2="Intercept/Subject", max_vif = as.character(vifs[["mod6.1"]]))
minfo[["mod7"]] <- c(fixed="Trial, RT, Phase, Previous Choice, trials since reversal", ixns = "previous choice x task phase x previous feedback, task phase x since reversal", l2="Intercept/Subject", max_vif = as.character(vifs[["mod7"]]))
minfo[["mod7.1"]] <- c(fixed="Trial, RT, Phase, Previous Choice, trials since reversal", ixns = "previous choice x task phase x previous feedback, task phase x since reversal", l2="Intercept/Subject", max_vif = as.character(vifs[["mod7.1"]]))
minfo[["mod8"]] <- c(fixed="Trial, RT, Phase, Previous Choice, trials since reversal", ixns = "previous choice x task phase, task_phase x previous feedback, task phase x since reversal, previous choice x since_reversal", l2="Intercept/Subject", max_vif = as.character(vifs[["mod8"]]))

minfo[["mod9"]] <- c(fixed="Trial, RT, Phase, Previous Choice, trials since reversal", ixns = "previous choice x task phase, task_phase x previous feedback, task phase x since reversal, previous choice x since_reversal, previous choice x previous feedback", l2="Intercept/Subject", max_vif = as.character(vifs[["mod9"]]))

#minfo[["mod9.0"]] <- c(fixed="Trial, RT, Phase, Previous Choice, trials since reversal", ixns = "task_phase x previous feedback x previous choice, task phase x since reversal, previous choice x since_reversal", l2="Intercept/Subject", max_vif = as.character(vifs[["mod9.0"]]))
minfo[["mod9.1"]] <- c(fixed="Trial, RT, Phase, Previous Choice, trials since reversal", ixns = "task_phase x previous feedback x previous choice, prev_choice x prevFeedback x task_phase, prevFeedback x since_reversal, previous choice x since_reversal", l2="Intercept/Subject",
max_vif = as.character(vifs[["mod9.1"]]))
minfo[["mod10"]] <- c(fixed="Trial, RT, Phase, Previous Choice, trials since reversal", ixns = "prev_choice*task_phase, task_phase*prevFeedback,  prev_choice*prevFeedback, prev_choice*since_reversal, task_phase*since_reversal", l2="Intercept/Subject", max_vif = as.character(vifs[["mod10"]]))
minfo[["mod11"]] <- c(fixed="Trial, RT, Phase, Previous Choice, trials since reversal", ixns = "prev_choice*task_phase*prevFeedback*since_reversal, task_phase*since_reversal", l2="Intercept/Subject", max_vif = vifs[["mod11"]])
minfo[["mod12"]] <- c(fixed="RT, Phase, Previous Choice, trials since reversal", ixns = "prev_choice*task_phase, task_phase*prevFeedback,  prev_choice*prevFeedback, prev_choice*since_reversal, task_phase*since_reversal", l2="Intercept/Subject", max_vif = as.character(vifs[["mod12"]]))

table_choose_rev_base <- dependlab::render_flex_aictab(minfo, mlist = modlist) %>% autofit()

save(table_choose_rev_base, data, choose_rev, mod10, stay_switch )

mod11 <- glmer(choose_A ~ (prev_choice+task_phase+since_reversal + prevFeedback)^2 + rt_log + prevrt_log + ( 1  | subject), family="binomial", control=glmerControl(optimizer = "bobyqa", optCtrl=list(maxfun=2e4)), nAGQ = 1, data = data, na.action = na.exclude)

summary(mod11)
car::vif(mod11)
AIC(mod10, mod11)

```


```{r}

mod12 <- glmer(choose_A ~ prev_choice*task_phase*since_reversal*prevFeedback + rt_log + prevrt_log + ( 1  | subject), family="binomial", control=glmerControl(optimizer = "bobyqa", optCtrl=list(maxfun=2e4)), nAGQ = 1, data = data, na.action = na.exclude)

summary(mod12)
car::vif(mod12)
AIC(mod12, mod11)

```






